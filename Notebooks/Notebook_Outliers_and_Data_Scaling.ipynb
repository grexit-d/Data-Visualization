{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Python Notebook: Outliers and Data Scaling"
   ],
   "metadata": {
    "id": "hYuOcdQ7I4Nv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Outliers\n",
    "**Definition**:\n",
    "Outliers are data points that significantly deviate from the majority of data in a dataset.\n",
    "They can skew results, distort statistical analyses, and impact machine learning models.\n"
   ],
   "metadata": {
    "id": "UxHPtMWNI6R0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Generate random data with an outlier\n",
    "data = np.random.normal(loc=50, scale=10, size=100)\n",
    "data = np.append(data, [150])  # Add an outlier\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(data)\n",
    "plt.title(\"Boxplot with Outlier\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "mkZT3Q0AJDNX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise: Generate your own data and visualize it\n",
    "\"\"\"\n",
    "Task:\n",
    "1. Generate a dataset with a mean of 30 and standard deviation of 5.\n",
    "2. Add an outlier to the dataset.\n",
    "3. Plot the data using a boxplot.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "elWNtnvMJQq4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Finding Outliers**"
   ],
   "metadata": {
    "id": "cP2npw1VL8fp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Method 1: Interquartile Range (IQR)\n",
    "def identify_outliers_iqr(data):\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = stats.iqr(data)\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
    "    return outliers\n",
    "\n",
    "\n",
    "outliers_iqr = identify_outliers_iqr(data)\n",
    "print(f\"Outliers using IQR: {outliers_iqr}\")"
   ],
   "metadata": {
    "id": "lrOt1Ng3Jnox"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise: Apply IQR to your dataset\n",
    "\"\"\"\n",
    "Task:\n",
    "1. Rewrite the function identify_outliers_iqr without using 'np.percentile()' and 'stats.iqr()'\n",
    "2. Compute the IQR for your generated dataset.\n",
    "3. Print the outliers.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "xRT5RrlVJrLD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Method 2: Z-Score\n",
    "def identify_outliers_zscore(data):\n",
    "    mean = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    z_scores = stats.zscore(data)  # Calculate Z-scores using scipy\n",
    "    outliers = [data[i] for i, z in enumerate(z_scores) if abs(z) > 3]\n",
    "    return outliers\n",
    "\n",
    "\n",
    "outliers_z = identify_outliers_zscore(data)\n",
    "print(f\"Outliers using Z-Score: {outliers_z}\")"
   ],
   "metadata": {
    "id": "EMcHymJnJtMK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise: Apply ZScore to your dataset\n",
    "\"\"\"\n",
    "Task:\n",
    "1. Rewrite the function identify_outliers_zscore without using 'np.mean()' and 'np.std()' and 'stats.zscore()'\n",
    "2. Compute the ZScore for your generated dataset.\n",
    "3. Print the outliers.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "FDf7z8sNKeTZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Handling Outliers**"
   ],
   "metadata": {
    "id": "jtqzN16SME2e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Strategy 1: Removing Outliers\n",
    "filtered_data = [x for x in data if x not in outliers_iqr]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(filtered_data)\n",
    "plt.title(\"Boxplot After Removing Outliers\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "gp7u7rB2MMjD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise: Remove outliers from your dataset and visualize the result\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "fvEJUwbsMQgc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Strategy 2: Capping Outliers\n",
    "def cap_outliers(data):\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return [\n",
    "        lower_bound if x < lower_bound else upper_bound if x > upper_bound else x\n",
    "        for x in data\n",
    "    ]\n",
    "\n",
    "\n",
    "capped_data = cap_outliers(data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(capped_data)\n",
    "plt.title(\"Boxplot After Capping Outliers\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "F-FQbrALMR3-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise: Cap outliers in your dataset and visualize the result\n",
    "\n"
   ],
   "metadata": {
    "id": "nCOZpwz0MX0s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Exercise (hard)\n",
    "\"\"\"\n",
    "In this exercise, we will detect and remove outliers based on logical rules rather than statistical methods.\n",
    "The dataset consists of transaction records with customer age, purchase amount, number of items,\n",
    "and membership type (Regular or Premium). Some transactions may be valid high-value purchases,\n",
    "while others may be suspicious depending on the context.\n",
    "\n",
    "Task\n",
    "1. Identify suspicious transactions that don't align with typical customer behavior.\n",
    "2. Implement rule-based outlier detection:\n",
    "    - High purchase amounts from Regular members are more suspicious than from Premium members.\n",
    "    - Younger customers making very large purchases could indicate an anomaly.\n",
    "    - A single-item purchase with an unusually high amount is suspicious.\n",
    "3. Extend the function to allow dynamic thresholds based on membership type and overall spending trends.\n",
    "\n",
    "Example dataset:\n",
    "transactions = [\n",
    "    {\"id\": 101, \"age\": 25, \"purchase_amount\": 50, \"num_items\": 2, \"membership\": \"Regular\"},\n",
    "    {\"id\": 102, \"age\": 65, \"purchase_amount\": 1000, \"num_items\": 1, \"membership\": \"Premium\"},\n",
    "    {\"id\": 103, \"age\": 19, \"purchase_amount\": 5000, \"num_items\": 1, \"membership\": \"Regular\"},  # Suspicious\n",
    "    {\"id\": 104, \"age\": 40, \"purchase_amount\": 120, \"num_items\": 5, \"membership\": \"Regular\"},\n",
    "    {\"id\": 105, \"age\": 30, \"purchase_amount\": 20000, \"num_items\": 1, \"membership\": \"Regular\"}, # Suspicious\n",
    "    {\"id\": 106, \"age\": 50, \"purchase_amount\": 300, \"num_items\": 10, \"membership\": \"Premium\"},\n",
    "    {\"id\": 107, \"age\": 75, \"purchase_amount\": 4000, \"num_items\": 2, \"membership\": \"Premium\"},\n",
    "]\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "1zKdcuqkImBa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Scaling"
   ],
   "metadata": {
    "id": "_79g2bVsMdIf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standardization"
   ],
   "metadata": {
    "id": "I-d5ZUdoMkOT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(np.array(data).reshape(-1, 1))  # Reshape for single feature\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(standardized_data, bins=20, alpha=0.7, label=\"Standardized Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Histogram of Standardized Data\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "CskWG4vmMpVH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Exercise:\n",
    "\"\"\"\n",
    "  1. Write your own standardization function instead of using the StandardScaler\n",
    "  2. Standardize your dataset and visualize it\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "m7i5eGdgMZdc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data = scaler.fit_transform(np.array(data).reshape(-1, 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(normalized_data, bins=20, alpha=0.7, label=\"Normalized Data\")\n",
    "plt.legend()\n",
    "plt.title(\"Histogram of Normalized Data\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "SeltmidbNC9i"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Exercise:\n",
    "\"\"\"\n",
    "  1. Write your own normalization function instead of using the MinMaxScaler\n",
    "  2. Normalize your dataset and visualize it\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "7-jV6npSNTVM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMvR3yWUI0BM"
   },
   "outputs": [],
   "source": [
    "\n",
    "## Exercise\n",
    "\"\"\"\n",
    "Experiment with other scaling techniques and normalization techniques and write down your findings.\n",
    "https://scikit-learn.org/1.5/modules/preprocessing.html\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "## Exercise\n",
    "\"\"\"\n",
    "  1. Load the Iris Dataset\n",
    "  2. Remove Outliers\n",
    "  3. Apply Standardization and/or Normalization (which one is best suited?)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# your turn ..."
   ],
   "metadata": {
    "id": "2SWb_qdBORev"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
